{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f69a2a",
   "metadata": {},
   "source": [
    "# LangChain Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affde1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip installs the packages in the base environment\n",
    "# pip - installs the packages in the virtual environment\n",
    "!pip install -r ./python_requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3bcc399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\r\n",
      "Version: 0.0.300\r\n",
      "Summary: Building applications with LLMs through composability\r\n",
      "Home-page: https://github.com/langchain-ai/langchain\r\n",
      "Author: \r\n",
      "Author-email: \r\n",
      "License: MIT\r\n",
      "Location: /Users/sm/miniconda3/envs/llm/lib/python3.11/site-packages\r\n",
      "Requires: aiohttp, anyio, dataclasses-json, jsonpatch, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f17fea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To upgrae a library\n",
    "!pip install langchain --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551fbc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\r\n",
      "Version: 0.0.300\r\n",
      "Summary: Building applications with LLMs through composability\r\n",
      "Home-page: https://github.com/langchain-ai/langchain\r\n",
      "Author: \r\n",
      "Author-email: \r\n",
      "License: MIT\r\n",
      "Location: /Users/sm/miniconda3/envs/llm/lib/python3.11/site-packages\r\n",
      "Requires: aiohttp, anyio, dataclasses-json, jsonpatch, langsmith, numexpr, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b87f2f",
   "metadata": {},
   "source": [
    "### Python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abed6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to dotenv we used in vs code\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "\n",
    "os.environ.get('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c426be",
   "metadata": {},
   "source": [
    "### LLM Models (Wrappers): GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afdba992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'text-davinci-003', 'temperature': 0.7, 'max_tokens': 512, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name='text-davinci-003',temperature=0.7,max_tokens=512)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64e5567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Quantum mechanics is a branch of physics that describes the behavior of matter and energy at the atomic and subatomic levels.\n"
     ]
    }
   ],
   "source": [
    "output = llm(\"explain quantum mechanics in one sentence\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8244b9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(llm.get_num_tokens(\"explain quantum mechanics in one sentence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a3e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_model = ['... is the capital of France.','What is the formula for the area of rectangle?']\n",
    "output = llm.generate(input_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0555ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer for  ... is the capital of France.  is  \n",
      "\n",
      "Paris.  \n",
      "\n",
      "Answer for  What is the formula for the area of rectangle?  is  \n",
      "\n",
      "The formula for the area of a rectangle is A = l x w, where A is the area, l is the length, and w is the width.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, generation in enumerate(output.generations):\n",
    "    print('Answer for ',input_to_model[idx],' is ', generation[0].text,' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec835b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='\\n\\n\"Experience the flavors of India - A Taste of Tradition!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Experience the Taste of India - Authentic Flavors from Across the Subcontinent\"', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n\"Authentic Flavors of India, Crafted with Love!\"', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'prompt_tokens': 24, 'completion_tokens': 49, 'total_tokens': 73}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('74715da3-fb58-4c69-b758-cb34c22411a4')), RunInfo(run_id=UUID('2a343ac4-479a-4c5a-ba71-9f60f51e784f')), RunInfo(run_id=UUID('ca69cafd-0b22-4024-8e48-b42328435f9b'))]\n"
     ]
    }
   ],
   "source": [
    "output = llm.generate(['Write an original tagline for Indian Restaurant']*3) # 3 different taglines\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d1b5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Experience the flavors of India - A Taste of Tradition!\"\n",
      "\n",
      "\n",
      "\"Experience the Taste of India - Authentic Flavors from Across the Subcontinent\"\n",
      "\n",
      "\n",
      "\"Authentic Flavors of India, Crafted with Love!\"\n"
     ]
    }
   ],
   "source": [
    "for idx, generation in enumerate(output.generations):\n",
    "    print(generation[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d54bb1",
   "metadata": {},
   "source": [
    "### ChatModels: GPT 3.5 (Turbo) & GPT 4\n",
    "- Variations to the usual language models\n",
    "- Rather exposing text in and text out kinda interface they expose conversation in and conversation out kinda of interface.\n",
    "- Reference [Click here for documentation](https://platform.openai.com/docs/guides/gpt/chat-completions-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7082582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0d67ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.5,max_tokens=1024)\n",
    "messages = [\n",
    "    SystemMessage(content='You are a mathematician and respond only in English.'),\n",
    "    HumanMessage(content='Explain discrete mathematics in one sentence')\n",
    "]\n",
    "output = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32251374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete mathematics is the branch of mathematics that deals with mathematical structures and objects that are fundamentally separate and distinct, rather than continuous.\n"
     ]
    }
   ],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca9e3b0",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "- Prompt refers to the input to the LLM models.\n",
    "- PTs are a way to create dynamic prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76c0211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19cb3f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['dish', 'language'] output_parser=None partial_variables={} template='You are an experienced Chef. \\nWrite a reviews about the {dish} in {language}.' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "template = '''You are an experienced Chef. \n",
    "Write a reviews about the {dish} in {language}.'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['dish','language']\n",
    "    ,template=template)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a16acbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Chicken Murgh Musallam is an amazing dish! The flavors of the marinade are perfectly balanced, providing a delightful combination of savory and sweet. The chicken is tender and juicy, and the spices add a delicious depth of flavor. The accompanying sauce is creamy and flavorful, adding a nice richness to the dish. Overall, I highly recommend Chicken Murgh Musallam to anyone looking for a delicious and unique dish.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model='text-davinci-003',temperature=0.7)\n",
    "output = llm(prompt.format(dish='Chicken Murgh Musallam',language='English'))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c4501",
   "metadata": {},
   "source": [
    "### Simple Chains\n",
    "- LLMs are often limited to what they can do on their own.\n",
    "- For simple task you can use LLMs on their own but for complex task they are not ideal.\n",
    "- Chains allow us to combine multiple components together to create a single coherent application.\n",
    "- EX: We can create a chain that takes user input, formats it with a prompt template and then passed the formatted response to an LLM.\n",
    "\n",
    "### Types of Chains\n",
    "- Simple \n",
    "- Sequential\n",
    "- Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f08cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo',temperature=0.5)\n",
    "\n",
    "template1 = '''You are an experience virologist.\n",
    "Write a few sentences about the following {virus} in {language}.'''\n",
    "\n",
    "template2 = '''You are an experienced Chef. \n",
    "Write a short review about the {dish} in English.'''\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=['virus','language']\n",
    "    ,template=template1)\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=['dish']\n",
    "    ,template=template2)\n",
    "\n",
    "chain1 = LLMChain(llm=llm,prompt=prompt1)\n",
    "chain2 = LLMChain(llm=llm,prompt=prompt2)\n",
    "\n",
    "\n",
    "# If the prompt template has two variables you create \n",
    "# a dictionary(object)like this to pass to the template.\n",
    "output1 = chain1.run({'virus':'HSV','language':'French'})\n",
    "\n",
    "# But if the prompt template only has 1 variable then you just pass\n",
    "# string variable like this\n",
    "\n",
    "output2 = chain2.run('Rogan Josh');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ec9dacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'HSV, ou Herpes Simplex Virus, est un virus couramment rencontré chez les êtres humains. Il existe deux types d'HSV : le HSV-1, responsable principalement des infections buccales et labiales, et le HSV-2, responsable des infections génitales. Ces virus peuvent provoquer des éruptions cutanées douloureuses et récurrentes, ainsi que des lésions sur les muqueuses. Bien qu'il n'existe pas de traitement curatif, des médicaments antiviraux peuvent aider à réduire la fréquence et la gravité des poussées d'herpès. La prévention des contacts directs avec les lésions et l'utilisation de préservatifs peuvent également aider à réduire la transmission de l'HSV.\n"
     ]
    }
   ],
   "source": [
    "print(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a61eefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A Taste of Authenticity: Rogan Josh\n",
      "\n",
      "As an experienced chef, I have had the privilege of indulging in a wide array of culinary delights from around the world. However, few dishes captivate my senses quite like the timeless Indian classic, Rogan Josh. This aromatic and richly flavored dish has earned its reputation as one of the crown jewels of Indian cuisine.\n",
      "\n",
      "Originating from the beautiful region of Kashmir, Rogan Josh is a slow-cooked lamb curry that effortlessly marries a symphony of spices, creating a truly unforgettable taste experience. The dish derives its name from the Persian words \"rogan\" meaning oil and \"josh\" meaning heat, perfectly encapsulating the essence of this fiery yet balanced creation.\n",
      "\n",
      "What sets Rogan Josh apart is its unique blend of spices, including Kashmiri red chili, ginger, fennel, and a medley of other aromatic ingredients. The result is a deep, luscious gravy that envelops tender pieces of lamb, infusing them with a harmonious blend of flavors that dance on the palate.\n",
      "\n",
      "The slow-cooking process is crucial to the success of Rogan Josh, allowing the meat to tenderize and absorb the flavors of the spices. This method ensures that every bite is an explosion of taste, with the lamb melting effortlessly in your mouth.\n",
      "\n",
      "The vibrant red hue of Rogan Josh is a visual feast in itself, a testament to the generous use of Kashmiri red chili. However, do not be deceived by its fiery appearance, as the heat is balanced by the addition of yogurt, which imparts a subtle creamy undertone to the dish.\n",
      "\n",
      "Traditionally served with fragrant basmati rice or naan bread, Rogan Josh is a dish that brings people together. Its rich flavors and comforting aroma evoke a sense of warmth and nostalgia, making it a perfect choice for a cozy family meal or a special occasion.\n",
      "\n",
      "Whether you are a seasoned lover of Indian cuisine or a curious gastronome looking to embark on a flavor-filled journey, Rogan Josh is a dish that should not be missed. Its authenticity, depth of flavors, and the sheer joy it brings to the table make it a true masterpiece that will leave you yearning for more.\n"
     ]
    }
   ],
   "source": [
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c78b3",
   "metadata": {},
   "source": [
    "### Simple Chains Conclusion:\n",
    "- We discussed simple chains which are the most basic type of chains.\n",
    "- They consist of a single LLM that is used to <ins>perform a single task</ins>.\n",
    "- **Ex**: Simple chain used to generate a text by passing a prompt to a LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc4e90",
   "metadata": {},
   "source": [
    "### Sequential Chains\n",
    "<img src=\"./images/sequential-chains.png\" style=\"border-radius:5px\"></img>\n",
    "\n",
    "Ex: Simple Sequential Chain\n",
    "\n",
    "**Input1** --> | LLM 1 |--> **Output 1**/**Input 2** --> | LLM 2| --> **Output 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2808919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "public static double linearRegression(List<Double> x, List<Double> y) {\n",
      "    // Check if the two lists are of equal lengths\n",
      "    if (x.size() != y.size()) {\n",
      "        throw new IllegalArgumentException(\"x and y must have the same number of elements\");\n",
      "    }\n",
      "\n",
      "    // Calculate the mean of x and y\n",
      "    double sumX = 0;\n",
      "    double sumY = 0;\n",
      "    for (int i = 0; i < x.size(); i++) {\n",
      "        sumX += x.get(i);\n",
      "        sumY += y.get(i);\n",
      "    }\n",
      "    double meanX = sumX / x.size();\n",
      "    double meanY = sumY / y.size();\n",
      "\n",
      "    // Calculate the slope of the linear regression line \n",
      "    double numerator = 0;\n",
      "    double denominator = 0;\n",
      "    for (int i = 0; i < x.size(); i++) {\n",
      "        numerator += (x.get(i) - meanX) * (y.get(i) - meanY);\n",
      "        denominator += (x.get(i) - meanX) * (x.get(i) - meanX);\n",
      "    }\n",
      "    double slope = numerator / denominator;\n",
      "\n",
      "    // Calculate the y-intercept of the linear regression line\n",
      "    double yIntercept = meanY - (slope * meanX);\n",
      "\n",
      "    // Return the slope and y-intercept\n",
      "    return slope + yIntercept;\n",
      "}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe provided Java function `linearRegression` calculates the slope and y-intercept of a linear regression line based on input lists `x` and `y`. \n",
      "\n",
      "Firstly, the function checks if the sizes of lists `x` and `y` are equal. If they are not equal, an `IllegalArgumentException` is thrown since linear regression requires equal numbers of x and y values. \n",
      "\n",
      "Next, the function calculates the means of lists `x` and `y` by summing all the values in each list and dividing by their respective sizes. This is done using a loop that iterates through the lists and updates two accumulator variables `sumX` and `sumY`. The means are then calculated by dividing `sumX` and `sumY` by the size of `x` and `y`, respectively.\n",
      "\n",
      "After calculating the means, the function proceeds to calculate the slope of the linear regression line. This is done using two separate accumulators `numerator` and `denominator` that are updated in a loop. In each iteration, the values `(x.get(i) - meanX)` and `(y.get(i) - meanY)` are multiplied and added to `numerator`, while `(x.get(i) - meanX)^2` is added to `denominator`.\n",
      "\n",
      "Subsequently, the y-intercept of the linear regression line is calculated by subtracting the product of the slope and mean X from the mean Y.\n",
      "\n",
      "Finally, the function returns the sum of the slope and the y-intercept.\n",
      "\n",
      "In summary, the `linearRegression` function performs a linear regression analysis on two provided lists `x` and `y` to determine the slope and y-intercept of the corresponding linear regression line.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain,SimpleSequentialChain\n",
    "\n",
    "llm1 = OpenAI(model_name='text-davinci-003',temperature=0.7,max_tokens=1024)\n",
    "prompt1= PromptTemplate(\n",
    "    input_variables=['concept'],\n",
    "    template='''You are an experienced scientist and Java Programmer.\n",
    "    Write a function that implements the concept of {concept}.''')\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
    "\n",
    "llm2 = OpenAI(model_name='gpt-3.5-turbo',temperature=1.2)\n",
    "prompt2= PromptTemplate(\n",
    "    input_variables=['function'],\n",
    "    template='''Given the Java Function {function}, \n",
    "    describe it as detailed as possible.''')\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2)\n",
    "\n",
    "# The order of specifying the chains in chains argument is very imp.\n",
    "# switch them and see how the output changes.\n",
    "simple_sequential_chain = SimpleSequentialChain(chains=[chain1,chain2], verbose = True)\n",
    "output = simple_sequential_chain.run('linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6266f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "public static double[] softmax(double[] values) {\n",
      "    double[] output = new double[values.length];\n",
      "    double sum = 0;\n",
      "    for (int i = 0; i < values.length; i++) {\n",
      "        output[i] = Math.exp(values[i]);\n",
      "        sum += output[i];\n",
      "    }\n",
      "    for (int i = 0; i < values.length; i++) {\n",
      "        output[i] /= sum;\n",
      "    }\n",
      "    return output;\n",
      "}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe given Java function is a method named \"softmax\" that takes in an array of double values as input and returns an array of double values as the output.\n",
      "\n",
      "First, it creates a new array called \"output\" with the same length as the input array, values. This array will store the results of the softmax operation for each element in the input array.\n",
      "\n",
      "Then, it initializes a variable named \"sum\" to zero. This variable will be used to store the sum of the exponential values of each element in the input array.\n",
      "\n",
      "Next, it iterates over each element in the input array using a for loop. Inside the loop, it calculates the exponential value of the current element using the Math.exp() method and assigns it to the corresponding index in the output array. It also adds this exponential value to the sum variable.\n",
      "\n",
      "After the first loop finishes, it has calculated the exponential values for each element and obtained the sum of all the exponential values.\n",
      "\n",
      "Then, it again iterates over each element in the output array using another for loop. Inside this loop, it divides each element in the output array by the sum variable. This step is necessary to normalize the values so that they form a valid probability distribution. Dividing each exponential value by the sum ensures that the sum of all the elements in the output array will be equal to 1.\n",
      "\n",
      "Finally, it returns the output array, which contains the softmax values for each element in the input array.\n",
      "\n",
      "In summary, the provided Java function calculates the softmax values for each element in the input array by first calculating their exponential values and then normalizing them by dividing each value by the sum of all the exponential values.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "output2 = simple_sequential_chain.run('softmax');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f61062",
   "metadata": {},
   "source": [
    "### LangChain Agents\n",
    "- Go to chat.openai.com and ask what is the answer to 5.1 ** 7.3\n",
    "- You will get wrong answer. Why ?\n",
    "- Because it is a complex operation and it cannot be done by AI model.\n",
    "- LLM are powerful tools but they are not perfect. \n",
    "\n",
    "- Task 2: Ask it anything about the most recent technology after year 2021. It will give you a made up/incorrect answer. (It was trained on data that cuts off in late 2021)\n",
    "\n",
    "- Conclusion: There are plugins that help LLM access browser and updated information, plugins are external tools that are similar to agents. One solution is to use Agents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00983484",
   "metadata": {},
   "source": [
    "### LangChain Agents in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e2ae52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "924b8b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to use the power operator and format the output\n",
      "Action: Python_REPL\n",
      "Action Input: print('{:.4f}'.format(5.1 ** 1.7))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m15.9540\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 15.9540\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'15.9540'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "agent_executor = create_python_agent(\n",
    "    llm=llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")\n",
    "# agent_executor.run('Calculate the square root of the factorial of 20 \\\n",
    "# and display it with 5 decimal points.')\n",
    "agent_executor.run('Calculate 5.1 ** 1.7 and display with 4 decimal points.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3528f8cf",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "<img src=\"./images/embeddings.png\" style=\"border-radius:5px\" />\n",
    "\n",
    "- The distance between 2 embeddings or 2 vectors measures their relatedness which translates to the relatedness between the text concepts they represent.\n",
    "- Similar embeddings or vectors represent similar concepts.\n",
    "\n",
    "<img src=\"./images/embeddings-example.png\" style=\"border-radius:5px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54101749",
   "metadata": {},
   "source": [
    "### Intro to Vector Databases\n",
    "\n",
    "**Challenges**\n",
    "<img src=\"./images/llm-challenges.png\"/>\n",
    "\n",
    "- LLMs, semantic search and AI requires a large amounts of data to train & operate.\n",
    "- Vector embeddings is a way of representing text as a set of numbers & numbers represent the meaning of the words in the text.\n",
    "- These embeddings if you try to store in csv or a local file, the size of the file will increase dramatically & performance will drop. Hence the need for vector database. (Ex: Pinecone, Chroma, Drant etc..)\n",
    "\n",
    "\n",
    "**Vector Database**\n",
    "<img src=\"./images/vector-db.png\"/>\n",
    "\n",
    "**SQL vs. Vector Databases**\n",
    "- SQL has a fixed schema while vector DB we apply similartiy methods to find a vector that is most similar to our search text.\n",
    "e- Vector databases use a combination of different optimized algorithms that all participate in **<ins>Approximate Nearest Neighbor (ANN)</ins>** search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e98fd",
   "metadata": {},
   "source": [
    "### How Vector DB works\n",
    "<img src=\"./images/vector-db-works.png\"/>\n",
    "\n",
    "- Vector embedding model is used for both storing and retrieving the vector from vector DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd66f11",
   "metadata": {},
   "source": [
    "### Splitting & Embedding text using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044f5fa0",
   "metadata": {},
   "source": [
    "[Continued over here..](./Vector-Database.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c4582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
